{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1c5c5-55e9-4cd7-ad85-dbb7d61f9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "#\n",
    "# Project:  Multivariate LISA\n",
    "#\n",
    "# Purposes: a) Multivariate Local Geary's c (Anselin, 2019)*\n",
    "#           b) Multivariate spatial clusters and outliers classification Mm_i (Oxoli, 2020)\n",
    "#\n",
    "# Improvements (Chen 2024** logic applied to multivariate):\n",
    "#   - Option to use GLOBAL (sum-)normalized symmetric weights (Chen-style canonical LISA consistency)\n",
    "#   - Canonical scaling with factor 1/(2k)\n",
    "#   - Two multivariate distances:\n",
    "#       (i) Euclidean in standardized space\n",
    "#       (ii) Mahalanobis in standardized space (covariance-aware)\n",
    "#\n",
    "# target data: Vector layer (GeoDataframe)\n",
    "#\n",
    "# Author:   Daniele Oxoli (daniele.oxoli@polimi.it)\n",
    "# Ref: Oxoli, D., Sabri, S., Rajabifard, A., & Brovelli, M. A. (2020). A classification technique for local multivariate clusters and outliers of spatial association. Transactions in GIS, 24(5), 1227-1247.\n",
    "# * Original Ref: Anselin, L. (2019). A local indicator of multivariate spatial association: extending Geary's C. Geographical Analysis, 51(2), 133-150.\n",
    "# ** Contrast Ref: Chen, Y. (2024). Reconstruction and normalization of LISA for spatial analysis. Plos one, 19(5), e0303456.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501820e8-5367-4514-b0f9-05f5cbbf31be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED PACKAGES\n",
    "import geopandas as gpd\n",
    "from libpysal import weights as psw\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import statsmodels.stats.multitest as mt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e910702b-6ddb-47e3-98c0-03c97c4ad5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# INPUT DATA and PARAMETERS\n",
    "# -----------------------------\n",
    "start = time.time()\n",
    "\n",
    "in_path = \"sample_data/demo_data.shp\"  # polygons with 2+ numeric fields\n",
    "out_path = \"sample_data/out.gpkg\" \n",
    "layer_name = \"multivariate_lisa_test\"   # name of the layer inside the gpkg\n",
    "\n",
    "df = gpd.read_file(in_path)\n",
    "\n",
    "# Analysis variables (as in attribute table)\n",
    "att_list = ['a', 'b', 'c', 'd']\n",
    "\n",
    "# Standardize attributes (sample SD, ddof=1) -> consistent with Geary convention\n",
    "att_list_norm = []\n",
    "for att in att_list:\n",
    "    df['n_' + att] = (df[att] - df[att].mean()) / df[att].std(ddof=1)\n",
    "    att_list_norm.append('n_' + att)\n",
    "\n",
    "# Inference setup\n",
    "permutations = 99        # use 99999 for final runs (very expensive)\n",
    "significance = 0.01      # for FDR alpha (<= 1/permutations recommended)\n",
    "np.random.seed(123)\n",
    "\n",
    "# Spatial weights (Queen contiguity)\n",
    "w = psw.Queen.from_dataframe(df)  # queen contiguity (edge or vertex)\n",
    "# Alternative:\n",
    "# w = psw.Rook.from_dataframe(df)\n",
    "\n",
    "# Build full weights matrix from ORIGINAL (binary) weights first\n",
    "w.transform = 'o'          # original; keep symmetric/binary structure for Queen\n",
    "wf_bin = w.full()[0].astype(float)\n",
    "\n",
    "# Ensure diagonal is zero (usually is, but be explicit)\n",
    "np.fill_diagonal(wf_bin, 0.0)\n",
    "\n",
    "# Neighbors dictionary\n",
    "neigh_dic = {i: w.neighbors[i] for i in df.index}\n",
    "\n",
    "# Fit normalized attributes into a matrix (n x k)\n",
    "att_mtx_norm = df[att_list_norm].to_numpy()\n",
    "n, k = att_mtx_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394e905b-f8bb-4fcb-899d-851cfecec55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# -----------------------------\n",
    "def global_sum_normalize(W):\n",
    "    \"\"\"Global (sum-)normalization: sum_{ij} w_ij = 1.\"\"\"\n",
    "    s = W.sum()\n",
    "    if s == 0:\n",
    "        raise ValueError(\"Weight matrix sum is zero; cannot global-normalize.\")\n",
    "    return W / s\n",
    "\n",
    "def row_standardize(W):\n",
    "    \"\"\"Row-standardization: sum_j w_ij = 1 for rows with at least one neighbor.\"\"\"\n",
    "    R = W.copy()\n",
    "    rs = R.sum(axis=1)\n",
    "    for i in range(R.shape[0]):\n",
    "        if rs[i] > 0:\n",
    "            R[i, :] /= rs[i]\n",
    "    return R\n",
    "\n",
    "def euclidean_d2(focal_vec, neigh_mtx):\n",
    "    \"\"\"\n",
    "    focal_vec: (k,)\n",
    "    neigh_mtx: (m, k)  neighbors' vectors\n",
    "    returns: (m,) squared Euclidean distances in standardized space\n",
    "    \"\"\"\n",
    "    diff = neigh_mtx - focal_vec\n",
    "    return np.sum(diff * diff, axis=1)\n",
    "\n",
    "def mahalanobis_d2(focal_vec, neigh_mtx, inv_cov):\n",
    "    \"\"\"\n",
    "    focal_vec: (k,)\n",
    "    neigh_mtx: (m, k)\n",
    "    inv_cov: (k, k) inverse covariance of standardized vectors\n",
    "    returns: (m,) squared Mahalanobis distances\n",
    "    \"\"\"\n",
    "    diff = neigh_mtx - focal_vec  # (m, k)\n",
    "    # quadratic form per row: diff @ inv_cov @ diff^T\n",
    "    return np.einsum('ij,jk,ik->i', diff, inv_cov, diff)\n",
    "\n",
    "def local_geary_multivariate(att_mtx, W, neigh_dict, distance=\"euclidean\", inv_cov=None, scale_canonical=True):\n",
    "    \"\"\"\n",
    "    Computes multivariate local Geary for each i:\n",
    "        C_i = (1/(2k)) * sum_j w_ij * d(i,j)^2   [canonical Chen-style]\n",
    "    If scale_canonical=False:\n",
    "        C_i = sum_j w_ij * d(i,j)^2             [legacy-like scaling]\n",
    "    distance: \"euclidean\" or \"mahalanobis\"\n",
    "    \"\"\"\n",
    "    n, k = att_mtx.shape\n",
    "    C = np.zeros(n, dtype=float)\n",
    "\n",
    "    for i in range(n):\n",
    "        js = neigh_dict[i]\n",
    "        if len(js) == 0:\n",
    "            C[i] = np.nan\n",
    "            continue\n",
    "\n",
    "        focal = att_mtx[i, :]\n",
    "        neigh_mtx = att_mtx[js, :]\n",
    "\n",
    "        if distance == \"euclidean\":\n",
    "            d2 = euclidean_d2(focal, neigh_mtx)\n",
    "        elif distance == \"mahalanobis\":\n",
    "            if inv_cov is None:\n",
    "                raise ValueError(\"inv_cov is required for Mahalanobis distance.\")\n",
    "            d2 = mahalanobis_d2(focal, neigh_mtx, inv_cov)\n",
    "        else:\n",
    "            raise ValueError(\"distance must be 'euclidean' or 'mahalanobis'\")\n",
    "\n",
    "        wij = W[i, js]\n",
    "        Ci = np.sum(wij * d2)\n",
    "\n",
    "        if scale_canonical:\n",
    "            Ci = (1.0 / (2.0 * k)) * Ci  # Chen-style canonical scaling\n",
    "\n",
    "        C[i] = Ci\n",
    "\n",
    "    return C\n",
    "\n",
    "def conditional_permutations_C(att_mtx_obs, neigh_dict, W, permutations, distance=\"euclidean\", inv_cov=None, scale_canonical=True):\n",
    "    \"\"\"\n",
    "    Conditional permutations, keeping focal attributes fixed and permuting attributes over locations\n",
    "    for neighbors. Returns:\n",
    "        C_sim: (n, permutations) simulated statistics for each i\n",
    "    \"\"\"\n",
    "    n, k = att_mtx_obs.shape\n",
    "    C_sim = np.zeros((n, permutations), dtype=float)\n",
    "\n",
    "    # Pre-split columns for fast permutation\n",
    "    cols = [att_mtx_obs[:, j].copy() for j in range(k)]\n",
    "\n",
    "    for p in range(permutations):\n",
    "        # permute each variable across all locations\n",
    "        perm_cols = [np.random.permutation(cols[j]) for j in range(k)]\n",
    "        perm_mtx = np.column_stack(perm_cols)  # (n, k)\n",
    "\n",
    "        # compute C_i for this permutation, conditioning on focal location's observed vector\n",
    "        for i in range(n):\n",
    "            js = neigh_dict[i]\n",
    "            if len(js) == 0:\n",
    "                C_sim[i, p] = np.nan\n",
    "                continue\n",
    "\n",
    "            focal = att_mtx_obs[i, :]       # keep focal observed\n",
    "            neigh_mtx = perm_mtx[js, :]     # neighbors permuted\n",
    "\n",
    "            if distance == \"euclidean\":\n",
    "                d2 = euclidean_d2(focal, neigh_mtx)\n",
    "            elif distance == \"mahalanobis\":\n",
    "                if inv_cov is None:\n",
    "                    raise ValueError(\"inv_cov is required for Mahalanobis distance.\")\n",
    "                d2 = mahalanobis_d2(focal, neigh_mtx, inv_cov)\n",
    "            else:\n",
    "                raise ValueError(\"distance must be 'euclidean' or 'mahalanobis'\")\n",
    "\n",
    "            wij = W[i, js]\n",
    "            Ci = np.sum(wij * d2)\n",
    "\n",
    "            if scale_canonical:\n",
    "                Ci = (1.0 / (2.0 * k)) * Ci\n",
    "\n",
    "            C_sim[i, p] = Ci\n",
    "\n",
    "    return C_sim\n",
    "\n",
    "def permutation_pvalues_two_sided(C_obs, C_sim):\n",
    "    \"\"\"\n",
    "    Two-sided permutation p-values using the \"extreme tail\" count approach.\n",
    "    Returns p_two_sided (n,)\n",
    "    \"\"\"\n",
    "    n, P = C_sim.shape\n",
    "    p = np.zeros(n, dtype=float)\n",
    "\n",
    "    for i in range(n):\n",
    "        sim = C_sim[i, :]\n",
    "        obs = C_obs[i]\n",
    "\n",
    "        if np.isnan(obs) or np.all(np.isnan(sim)):\n",
    "            p[i] = np.nan\n",
    "            continue\n",
    "\n",
    "        # one-sided count for greater-than\n",
    "        larger = np.sum(sim > obs)\n",
    "\n",
    "        # two-sided via min-tail\n",
    "        larger = min(larger, P - larger)\n",
    "        p[i] = (larger + 1.0) / (P + 1.0)\n",
    "\n",
    "    return 2.0 * p  # two-sided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae370df0-3e35-4956-8684-933909d66d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# (A) LEGACY-LIKE: Row-standardized + Euclidean\n",
    "#     (closest to your current implementation)\n",
    "#     NOTE: this does NOT enforce Chen's local-global canonical consistency.\n",
    "# ==========================================\n",
    "W_A = row_standardize(wf_bin)\n",
    "C_obs_A = local_geary_multivariate(\n",
    "    att_mtx=att_mtx_norm,\n",
    "    W=W_A,\n",
    "    neigh_dict=neigh_dic,\n",
    "    distance=\"euclidean\",\n",
    "    inv_cov=None,\n",
    "    scale_canonical=False   # legacy scaling (no 1/(2k))\n",
    ")\n",
    "df[\"C_ki_A\"] = C_obs_A\n",
    "\n",
    "# Normal approximation (optional; keep if you want)\n",
    "C_z_A = (C_obs_A - np.nanmean(C_obs_A)) / np.nanstd(C_obs_A)\n",
    "C_p_norm_A = st.norm.sf(np.abs(C_z_A)) * 2\n",
    "C_p_norm_fdr_A = mt.fdrcorrection(C_p_norm_A, alpha=significance)\n",
    "df[\"C_p_norm_A\"] = C_p_norm_A\n",
    "df[\"C_norm_fdr_A\"] = C_p_norm_fdr_A[1]\n",
    "df[\"C_z_norm_A\"] = C_z_A\n",
    "\n",
    "# Permutation inference\n",
    "C_sim_A = conditional_permutations_C(\n",
    "    att_mtx_obs=att_mtx_norm,\n",
    "    neigh_dict=neigh_dic,\n",
    "    W=W_A,\n",
    "    permutations=permutations,\n",
    "    distance=\"euclidean\",\n",
    "    inv_cov=None,\n",
    "    scale_canonical=False\n",
    ")\n",
    "E_A = np.nanmean(C_sim_A, axis=1)\n",
    "S_A = np.nanstd(C_sim_A, axis=1)\n",
    "df[\"C_z_sim_A\"] = (C_obs_A - E_A) / S_A\n",
    "p_two_A = permutation_pvalues_two_sided(C_obs_A, C_sim_A)\n",
    "df[\"C_p_sim_A\"] = p_two_A\n",
    "df[\"C_sim_fdr_A\"] = mt.fdrcorrection(p_two_A, alpha=significance)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbdd5b3-8ba7-4239-9c6a-5bdd8bb872ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# (B) CHEN-CANONICAL (GC3-style): Global sum-normalized + Euclidean + 1/(2k)\n",
    "#     This is the recommended Chen-consistent multivariate extension.\n",
    "# ==========================================\n",
    "\n",
    "W_B = global_sum_normalize(wf_bin)\n",
    "\n",
    "C_obs_B = local_geary_multivariate(\n",
    "    att_mtx=att_mtx_norm,\n",
    "    W=W_B,\n",
    "    neigh_dict=neigh_dic,\n",
    "    distance=\"euclidean\",\n",
    "    inv_cov=None,\n",
    "    scale_canonical=True    # includes 1/(2k)\n",
    ")\n",
    "\n",
    "df[\"C_ki_B\"] = C_obs_B\n",
    "\n",
    "# Permutation inference\n",
    "C_sim_B = conditional_permutations_C(\n",
    "    att_mtx_obs=att_mtx_norm,\n",
    "    neigh_dict=neigh_dic,\n",
    "    W=W_B,\n",
    "    permutations=permutations,\n",
    "    distance=\"euclidean\",\n",
    "    inv_cov=None,\n",
    "    scale_canonical=True\n",
    ")\n",
    "\n",
    "E_B = np.nanmean(C_sim_B, axis=1)\n",
    "S_B = np.nanstd(C_sim_B, axis=1)\n",
    "\n",
    "df[\"C_z_sim_B\"] = (C_obs_B - E_B) / S_B\n",
    "\n",
    "p_two_B = permutation_pvalues_two_sided(C_obs_B, C_sim_B)\n",
    "\n",
    "df[\"C_p_sim_B\"] = p_two_B\n",
    "df[\"C_sim_fdr_B\"] = mt.fdrcorrection(p_two_B, alpha=significance)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b448ee6-7619-454b-804b-0178a43307b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# (C) CHEN-CANONICAL (GC3-style): Global sum-normalized + Mahalanobis + 1/(2k)\n",
    "#     Covariance-aware multivariate differences.\n",
    "# ==========================================\n",
    "\n",
    "W_C = global_sum_normalize(wf_bin)\n",
    "\n",
    "# Sample covariance of standardized vectors (k x k), and its (pseudo-)inverse\n",
    "Sigma_z = np.cov(att_mtx_norm, rowvar=False, bias=False)   # sample covariance (ddof=1)\n",
    "inv_Sigma_z = np.linalg.pinv(Sigma_z)                      # robust inverse (handles singularity)\n",
    "\n",
    "C_obs_C = local_geary_multivariate(\n",
    "    att_mtx=att_mtx_norm,\n",
    "    W=W_C,\n",
    "    neigh_dict=neigh_dic,\n",
    "    distance=\"mahalanobis\",\n",
    "    inv_cov=inv_Sigma_z,\n",
    "    scale_canonical=True\n",
    ")\n",
    "\n",
    "df[\"C_ki_C\"] = C_obs_C\n",
    "\n",
    "# Permutation inference (same inverse covariance; focal fixed; neighbors permuted)\n",
    "C_sim_C = conditional_permutations_C(\n",
    "    att_mtx_obs=att_mtx_norm,\n",
    "    neigh_dict=neigh_dic,\n",
    "    W=W_C,\n",
    "    permutations=permutations,\n",
    "    distance=\"mahalanobis\",\n",
    "    inv_cov=inv_Sigma_z,\n",
    "    scale_canonical=True\n",
    ")\n",
    "\n",
    "E_C = np.nanmean(C_sim_C, axis=1)\n",
    "S_C = np.nanstd(C_sim_C, axis=1)\n",
    "\n",
    "df[\"C_z_sim_C\"] = (C_obs_C - E_C) / S_C\n",
    "\n",
    "p_two_C = permutation_pvalues_two_sided(C_obs_C, C_sim_C)\n",
    "\n",
    "df[\"C_p_sim_C\"] = p_two_C\n",
    "df[\"C_sim_fdr_C\"] = mt.fdrcorrection(p_two_C, alpha=significance)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67723be7-5e45-4917-8e44-7309a6ce3343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Mm_i CLASSIFICATION (unchanged logic; choose which C_ki to use)\n",
    "# -----------------------------\n",
    "# Pick the statistic column produced by the option you use:\n",
    "C_col = \"C_ki_A\"   # or \"C_ki_B\" or \"C_ki_C\"\n",
    "\n",
    "class_Cki = []\n",
    "Mmc_Di_ref = df[att_list_norm].median().mean()\n",
    "\n",
    "for i in df.index:\n",
    "    js = neigh_dic[i]\n",
    "    loi = np.hstack((i, js))  # i plus neighbors\n",
    "\n",
    "    # candidate cluster if local Geary is \"low\"\n",
    "    if df[C_col].iloc[i] <= df[C_col].mean():\n",
    "        Mmc_i = df[att_list_norm].iloc[loi, :].median().mean()\n",
    "        if Mmc_i > Mmc_Di_ref:\n",
    "            class_Cki.append('hh')\n",
    "        else:\n",
    "            class_Cki.append('ll')\n",
    "    else:\n",
    "        # candidate outlier\n",
    "        Mmo_Di_ref = df[att_list_norm].iloc[js, :].median().mean()\n",
    "        Mmo_i = df[att_list_norm].iloc[i, :].mean()\n",
    "        if Mmo_i > Mmo_Di_ref:\n",
    "            class_Cki.append('hl')\n",
    "        else:\n",
    "            class_Cki.append('lh')\n",
    "\n",
    "df[\"class_Cki\"] = class_Cki\n",
    "\n",
    "# Save output\n",
    "df.to_file(out_path, layer=layer_name, driver=\"GPKG\")\n",
    "\n",
    "end_ci = time.time()\n",
    "print(\"time_ci\")\n",
    "print(end_ci - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e675c82-5277-4f7f-85b8-42743acd87b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Mm_i CLASSIFICATION FOR ALL AVAILABLE C_ki_* COLUMNS\n",
    "# -----------------------------\n",
    "\n",
    "# List of possible statistic columns\n",
    "C_columns = [\"C_ki_A\", \"C_ki_B\", \"C_ki_C\"]\n",
    "\n",
    "# Reference for cluster classification (global multivariate median)\n",
    "Mmc_Di_ref = df[att_list_norm].median().mean()\n",
    "\n",
    "for C_col in C_columns:\n",
    "\n",
    "    if C_col not in df.columns:\n",
    "        continue  # skip if this statistic was not computed\n",
    "\n",
    "    class_list = []\n",
    "\n",
    "    # mean threshold specific to this statistic\n",
    "    C_mean = df[C_col].mean()\n",
    "\n",
    "    for i in df.index:\n",
    "        js = neigh_dic[i]\n",
    "        loi = np.hstack((i, js))  # focal + neighbors\n",
    "\n",
    "        # candidate cluster (low Geary)\n",
    "        if df[C_col].iloc[i] <= C_mean:\n",
    "\n",
    "            Mmc_i = df[att_list_norm].iloc[loi, :].median().mean()\n",
    "\n",
    "            if Mmc_i > Mmc_Di_ref:\n",
    "                class_list.append('hh')\n",
    "            else:\n",
    "                class_list.append('ll')\n",
    "\n",
    "        # candidate spatial outlier (high Geary)\n",
    "        else:\n",
    "\n",
    "            Mmo_Di_ref = df[att_list_norm].iloc[js, :].median().mean()\n",
    "            Mmo_i = df[att_list_norm].iloc[i, :].mean()\n",
    "\n",
    "            if Mmo_i > Mmo_Di_ref:\n",
    "                class_list.append('hl')\n",
    "            else:\n",
    "                class_list.append('lh')\n",
    "\n",
    "    # store result using suffix of statistic (more robust extraction)\n",
    "    suffix = C_col.replace(\"C_ki_\", \"\")\n",
    "    df[f\"class_Cki_{suffix}\"] = class_list\n",
    "\n",
    "\n",
    "# Save output\n",
    "df.to_file(driver='ESRI Shapefile', filename=out_path)\n",
    "\n",
    "end_ci = time.time()\n",
    "print(\"time_ci\")\n",
    "print(end_ci - start)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1170adab-abcb-42fd-a3aa-0b0c57746359",
   "metadata": {},
   "source": [
    "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_29228\\1229444984.py:52: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
    "  df.to_file(driver='ESRI Shapefile', filename=out_path)\n",
    "C:\\Users\\user\\anaconda3\\envs\\multivariate-lisa\\Lib\\site-packages\\pyogrio\\raw.py:733: RuntimeWarning: Normalized/laundered field name: 'C_norm_fdr_A' to 'C_norm_fdr'\n",
    "  ogr_write(\n",
    "C:\\Users\\user\\anaconda3\\envs\\multivariate-lisa\\Lib\\site-packages\\pyogrio\\raw.py:733: RuntimeWarning: Normalized/laundered field name: 'C_sim_fdr_A' to 'C_sim_fdr_'\n",
    "  ogr_write(\n",
    "C:\\Users\\user\\anaconda3\\envs\\multivariate-lisa\\Lib\\site-packages\\pyogrio\\raw.py:733: RuntimeWarning: Normalized/laundered field name: 'C_sim_fdr_B' to 'C_sim_fd_1'\n",
    "  ogr_write(\n",
    "C:\\Users\\user\\anaconda3\\envs\\multivariate-lisa\\Lib\\site-packages\\pyogrio\\raw.py:733: RuntimeWarning: Normalized/laundered field name: 'C_sim_fdr_C' to 'C_sim_fd_2'\n",
    "  ogr_write(\n",
    "C:\\Users\\user\\anaconda3\\envs\\multivariate-lisa\\Lib\\site-packages\\pyogrio\\raw.py:733: RuntimeWarning: Normalized/laundered field name: 'class_Cki_A' to 'class_Cki_'\n",
    "  ogr_write(\n",
    "C:\\Users\\user\\anaconda3\\envs\\multivariate-lisa\\Lib\\site-packages\\pyogrio\\raw.py:733: RuntimeWarning: Normalized/laundered field name: 'class_Cki_B' to 'class_Ck_1'\n",
    "  ogr_write(\n",
    "C:\\Users\\user\\anaconda3\\envs\\multivariate-lisa\\Lib\\site-packages\\pyogrio\\raw.py:733: RuntimeWarning: Normalized/laundered field name: 'class_Cki_C' to 'class_Ck_2'\n",
    "  ogr_write("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13d68f6-d6ae-4574-b9e5-c015da477676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "#     LOCAL–GLOBAL CONSISTENCY CHECKS\n",
    "#     Use this block AFTER computing any C_ki_* column.\n",
    "#\n",
    "#     Goal: verify the Chen-style canonical identity:\n",
    "#         sum_i C_i  ==  C_global\n",
    "#     where\n",
    "#         C_global = (1/(2k)) * sum_{i,j} w_ij * d(i,j)^2\n",
    "#\n",
    "#     Notes:\n",
    "#     - This identity is guaranteed when:\n",
    "#         * W is GLOBAL (sum-)normalized (sum_{i,j} w_ij = 1), and\n",
    "#         * the local statistic uses the same scaling (typically 1/(2k))\n",
    "#     - It generally FAILS for row-standardized weights (row sums = 1).\n",
    "# ==========================================\n",
    "\n",
    "def global_geary_multivariate(att_mtx, W, neigh_dict, distance=\"euclidean\", inv_cov=None, scale_canonical=True):\n",
    "    \"\"\"\n",
    "    Computes the corresponding GLOBAL multivariate Geary:\n",
    "        C_global = sum_i C_i\n",
    "    computed explicitly as:\n",
    "        C_global = (1/(2k)) * sum_{i,j} w_ij * d(i,j)^2\n",
    "    If scale_canonical=False:\n",
    "        C_global = sum_{i,j} w_ij * d(i,j)^2\n",
    "    \"\"\"\n",
    "    n, k = att_mtx.shape\n",
    "    total = 0.0\n",
    "\n",
    "    for i in range(n):\n",
    "        js = neigh_dict[i]\n",
    "        if len(js) == 0:\n",
    "            continue\n",
    "\n",
    "        focal = att_mtx[i, :]\n",
    "        neigh_mtx = att_mtx[js, :]\n",
    "\n",
    "        if distance == \"euclidean\":\n",
    "            d2 = euclidean_d2(focal, neigh_mtx)\n",
    "        elif distance == \"mahalanobis\":\n",
    "            if inv_cov is None:\n",
    "                raise ValueError(\"inv_cov is required for Mahalanobis distance.\")\n",
    "            d2 = mahalanobis_d2(focal, neigh_mtx, inv_cov)\n",
    "        else:\n",
    "            raise ValueError(\"distance must be 'euclidean' or 'mahalanobis'\")\n",
    "\n",
    "        wij = W[i, js]\n",
    "        total += np.sum(wij * d2)\n",
    "\n",
    "    if scale_canonical:\n",
    "        total *= (1.0 / (2.0 * k))\n",
    "\n",
    "    return total\n",
    "\n",
    "\n",
    "def local_global_consistency_report(df, C_col, att_mtx, W, neigh_dict,\n",
    "                                    distance=\"euclidean\", inv_cov=None, scale_canonical=True,\n",
    "                                    print_first_n=10):\n",
    "    \"\"\"\n",
    "    Prints and returns a small report of local-global consistency.\n",
    "    Compares:\n",
    "        sum_i C_i (from df[C_col])\n",
    "    with:\n",
    "        C_global (explicit computation)\n",
    "    \"\"\"\n",
    "    # Sum of locals (ignore NaNs if islands exist)\n",
    "    sum_locals = np.nansum(df[C_col].to_numpy())\n",
    "\n",
    "    # Global computed from weights + distances\n",
    "    C_global = global_geary_multivariate(\n",
    "        att_mtx=att_mtx, W=W, neigh_dict=neigh_dict,\n",
    "        distance=distance, inv_cov=inv_cov, scale_canonical=scale_canonical\n",
    "    )\n",
    "\n",
    "    abs_diff = float(np.abs(sum_locals - C_global))\n",
    "    rel_diff = float(abs_diff / (np.abs(C_global) + 1e-15))\n",
    "\n",
    "    print(\"----- Local–Global Consistency Check -----\")\n",
    "    print(f\"Statistic column: {C_col}\")\n",
    "    print(f\"Distance: {distance}\")\n",
    "    print(f\"Canonical scaling (1/(2k)): {scale_canonical}\")\n",
    "    print(f\"Sum of locals   Σ_i C_i     = {sum_locals:.12g}\")\n",
    "    print(f\"Global computed C_global    = {C_global:.12g}\")\n",
    "    print(f\"Absolute diff              = {abs_diff:.12g}\")\n",
    "    print(f\"Relative diff              = {rel_diff:.12g}\")\n",
    "\n",
    "    # Basic weight diagnostics\n",
    "    print(\"\\n----- Weight diagnostics -----\")\n",
    "    print(f\"Sum of all weights Σ_ij w_ij = {W.sum():.12g}\")\n",
    "    row_sums = W.sum(axis=1)\n",
    "    print(f\"Row sums (min, mean, max)    = ({row_sums.min():.12g}, {row_sums.mean():.12g}, {row_sums.max():.12g})\")\n",
    "\n",
    "    # Show a few example locals vs their contribution\n",
    "    print(\"\\n----- First few local values -----\")\n",
    "    cvals = df[C_col].to_numpy()\n",
    "    idxs = np.where(~np.isnan(cvals))[0][:print_first_n]\n",
    "    for i in idxs:\n",
    "        print(f\"i={i}, C_i={cvals[i]:.12g}, deg={len(neigh_dict[i])}\")\n",
    "\n",
    "    return {\n",
    "        \"C_col\": C_col,\n",
    "        \"distance\": distance,\n",
    "        \"scale_canonical\": scale_canonical,\n",
    "        \"sum_locals\": sum_locals,\n",
    "        \"C_global\": C_global,\n",
    "        \"abs_diff\": abs_diff,\n",
    "        \"rel_diff\": rel_diff,\n",
    "        \"sum_weights\": float(W.sum()),\n",
    "        \"row_sum_min\": float(row_sums.min()),\n",
    "        \"row_sum_mean\": float(row_sums.mean()),\n",
    "        \"row_sum_max\": float(row_sums.max()),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9372ee77-c6fc-41c4-9966-d773d9262c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# After you select and run ONE of blocks (A), (B), or (C), call this with the matching settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c403da6-39db-4566-844e-cde922450dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Legacy row-standardized Euclidean (A) -> expect mismatch\n",
    "report_A = local_global_consistency_report(\n",
    "    df=df, C_col=\"C_ki_A\",\n",
    "    att_mtx=att_mtx_norm, W=W_A, neigh_dict=neigh_dic,\n",
    "    distance=\"euclidean\", inv_cov=None, scale_canonical=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65eb289-3666-4698-8842-46a0012e8da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Chen-canonical Euclidean (B) -> expect near-zero diff\n",
    "report_B = local_global_consistency_report(\n",
    "    df=df, C_col=\"C_ki_B\",\n",
    "    att_mtx=att_mtx_norm, W=W_B, neigh_dict=neigh_dic,\n",
    "    distance=\"euclidean\", inv_cov=None, scale_canonical=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c926c859-71fb-4d15-bfd9-0f4c6dbf9d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Chen-canonical Mahalanobis (C) -> expect near-zero diff\n",
    "report_C = local_global_consistency_report(\n",
    "    df=df, C_col=\"C_ki_C\",\n",
    "    att_mtx=att_mtx_norm, W=W_C, neigh_dict=neigh_dic,\n",
    "    distance=\"mahalanobis\", inv_cov=inv_Sigma_z, scale_canonical=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
